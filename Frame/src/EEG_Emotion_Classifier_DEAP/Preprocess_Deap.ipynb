{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Valence value regression based on Deap Dataset\n",
    "\n",
    "## 0. This notebook is based on DEAP database\n",
    "\n",
    "Anyone should refer to DEAP team first\n",
    "\n",
    "@article{koelstra2012deap,\n",
    "  title={Deap: A database for emotion analysis; using physiological signals},\n",
    "  author={Koelstra, Sander and Muhl, Christian and Soleymani, Mohammad and Lee, Jong-Seok and Yazdani, Ashkan and Ebrahimi, Touradj and Pun, Thierry and Nijholt, Anton and Patras, Ioannis},\n",
    "  journal={IEEE Transactions on Affective Computing},\n",
    "  volume={3},\n",
    "  number={1},\n",
    "  pages={18--31},\n",
    "  year={2012},\n",
    "  publisher={IEEE}\n",
    "}\n",
    "\n",
    "## 1. Dependency\n",
    "* numpy\n",
    "* scipy\n",
    "* mne (MNE-Python)\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "# from scipy import signal\n",
    "import pickle as pickle\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "import os\n",
    "#import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Variables setup\n",
    "File Name data\\SXX.dat, XX \\in [0,31]\n",
    "* data: 40 x 40 x 8064: trial x channel x data\n",
    "* label: 40 x 4: video/trial x label (valence, arousal, dominance, liking)\n",
    "\n",
    "Channel Indice: {\n",
    "* 1 : AF3; 2: F3; 3: F7; 4: FC5; 7: T7; 11: P7; 13: O1\n",
    "* 17: AF4; 19: F4; 20: F8; 21: FC6; 25: T8; 29: P8; 31: O2 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel = [1,2,3,4,6,11,13,17,19,20,21,25,29,31] #14 Channels chosen to fit Emotiv Epoch+\n",
    "band = [4,8,12,16,25,45] #5 bands\n",
    "window_size = 256 #Averaging band power of 2 sec\n",
    "step_size = 16 #Each 0.125 sec update once\n",
    "sample_rate = 128 #Sampling rate of 128 Hz\n",
    "subjectList = ['01','02','03','04','05','06','07','08','09','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32']\n",
    "#List of subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FFT with MNE-Python\n",
    "* [4-8]: theta band\n",
    "* [8-12]: alpha band\n",
    "* [12-16]: low beta band \n",
    "* [16-25]: high beta band\n",
    "* [25-45]: gamma band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_power(X, band, sample_rate):\n",
    "    \"\"\"\n",
    "    计算频带功率，使用 MNE-Python 进行 EEG 信号处理\n",
    "    替代 pyeeg.bin_power，使用 MNE 的功率谱密度计算方法\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        输入信号 (1D array)\n",
    "    band : list\n",
    "        频带边界，例如 [4, 8, 12, 16, 25, 45]\n",
    "    sample_rate : int\n",
    "        采样率 (Hz)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    power : array\n",
    "        各频带的功率值\n",
    "    frequencies : array\n",
    "        频率数组（用于调试）\n",
    "    \"\"\"\n",
    "    # 将数据转换为 MNE 格式 (需要是 2D: n_channels x n_times)\n",
    "    X_2d = X.reshape(1, -1)  # 转换为 (1, n_samples)\n",
    "    \n",
    "    # 使用 MNE 计算功率谱密度\n",
    "    # psd_array_welch 返回 (psd, freqs)，其中 psd 形状为 (n_channels, n_freqs)\n",
    "    psd, frequencies = mne.time_frequency.psd_array_welch(\n",
    "        X_2d, \n",
    "        sfreq=sample_rate,\n",
    "        fmin=band[0],\n",
    "        fmax=band[-1],\n",
    "        n_fft=len(X),\n",
    "        n_overlap=0,\n",
    "        n_per_seg=len(X),\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # psd 形状是 (n_channels, n_freqs)，取第一个通道\n",
    "    psd = psd[0]\n",
    "    \n",
    "    # 计算每个频带的功率\n",
    "    power = []\n",
    "    for i in range(len(band) - 1):\n",
    "        # 找到频带范围内的频率索引\n",
    "        freq_mask = (frequencies >= band[i]) & (frequencies < band[i + 1])\n",
    "        if np.any(freq_mask):\n",
    "            # 使用梯形积分计算频带功率\n",
    "            band_power = np.trapezoid(psd[freq_mask], frequencies[freq_mask])\n",
    "            power.append(band_power)\n",
    "        else:\n",
    "            # 如果没有找到频率点，返回 0\n",
    "            power.append(0.0)\n",
    "    \n",
    "    return np.array(power), frequencies\n",
    "\n",
    "def FFT_Processing (sub, channel, band, window_size, step_size, sample_rate):\n",
    "    #data\\s01.dat\n",
    "    '''\n",
    "    arguments:  string subject\n",
    "                list channel indice\n",
    "                list band\n",
    "                int window size for FFT\n",
    "                int step size for FFT\n",
    "                int sample rate for FFT\n",
    "    return:     void\n",
    "    '''\n",
    "    meta = []\n",
    "    with open('data\\s' + sub + '.dat', 'rb') as file:\n",
    "\n",
    "        subject = pickle.load(file, encoding='latin1') #resolve the python 2 data problem by encoding : latin1\n",
    "\n",
    "        for i in range (0,40):\n",
    "            # loop over 0-39 trails\n",
    "            data = subject[\"data\"][i]\n",
    "            labels = subject[\"labels\"][i]\n",
    "            start = 0\n",
    "\n",
    "            while start + window_size < data.shape[1]:\n",
    "                meta_array = []\n",
    "                meta_data = [] #meta vector for analysis\n",
    "                for j in channel:\n",
    "                    X = data[j][start : start + window_size] #Slice raw data over 2 sec, at interval of 0.125 sec\n",
    "                    Y, _ = bin_power(X, band, sample_rate) #FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\n",
    "                    meta_data = meta_data + list(Y)\n",
    "\n",
    "                meta_array.append(np.array(meta_data))\n",
    "                meta_array.append(np.array(labels))  # 确保 labels 也是数组\n",
    "\n",
    "                meta.append(meta_array)  # 保持为列表，不转换为数组\n",
    "                start = start + step_size\n",
    "                \n",
    "        # 转换为数组时使用 object dtype 以支持不同形状的元素\n",
    "        meta = np.array(meta, dtype=object)\n",
    "        np.save('out\\s' + sub, meta, allow_pickle=True, fix_imports=True)\n",
    "\n",
    "def testing (M, L, model):\n",
    "    '''\n",
    "    arguments:  M: testing dataset\n",
    "                L: testing dataset label\n",
    "                model: scikit-learn model\n",
    "\n",
    "    return:     void\n",
    "    '''\n",
    "    output = model.predict(M[0:78080:32])\n",
    "    label = L[0:78080:32]\n",
    "\n",
    "    k = 0\n",
    "    l = 0\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        k = k + (output[i] - label[i])*(output[i] - label[i]) #square difference \n",
    "\n",
    "        #a good guess\n",
    "        if (output[i] > 5 and label[i] > 5):\n",
    "            l = l + 1\n",
    "        elif (output[i] < 5 and label[i] <5):\n",
    "            l = l + 1\n",
    "\n",
    "    print (\"l2 error:\", k/len(label), \"classification accuracy:\", l / len(label),l, len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。: 'C:\\\\Users\\\\29213\\\\AppData\\\\Roaming\\\\.mne'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subjects \u001b[38;5;129;01min\u001b[39;00m subjectList:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mFFT_Processing\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msubjects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 83\u001b[0m, in \u001b[0;36mFFT_Processing\u001b[1;34m(sub, channel, band, window_size, step_size, sample_rate)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m channel:\n\u001b[0;32m     82\u001b[0m     X \u001b[38;5;241m=\u001b[39m data[j][start : start \u001b[38;5;241m+\u001b[39m window_size] \u001b[38;5;66;03m#Slice raw data over 2 sec, at interval of 0.125 sec\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     Y, _ \u001b[38;5;241m=\u001b[39m \u001b[43mbin_power\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mband\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_rate\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#FFT over 2 sec of channel j, in seq of theta, alpha, low beta, high beta, gamma\u001b[39;00m\n\u001b[0;32m     84\u001b[0m     meta_data \u001b[38;5;241m=\u001b[39m meta_data \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(Y)\n\u001b[0;32m     86\u001b[0m meta_array\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39marray(meta_data))\n",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m, in \u001b[0;36mbin_power\u001b[1;34m(X, band, sample_rate)\u001b[0m\n\u001b[0;32m     23\u001b[0m X_2d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# 转换为 (1, n_samples)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 使用 MNE 计算功率谱密度\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# psd_array_welch 返回 (psd, freqs)，其中 psd 形状为 (n_channels, n_freqs)\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m psd, frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_frequency\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpsd_array_welch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_2d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mband\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mband\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_fft\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_overlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_per_seg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     36\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# psd 形状是 (n_channels, n_freqs)，取第一个通道\u001b[39;00m\n\u001b[0;32m     39\u001b[0m psd \u001b[38;5;241m=\u001b[39m psd[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m<decorator-gen-24>:10\u001b[0m, in \u001b[0;36mpsd_array_welch\u001b[1;34m(x, sfreq, fmin, fmax, n_fft, n_overlap, n_per_seg, n_jobs, average, window, remove_dc, output, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mne\\time_frequency\\psd.py:209\u001b[0m, in \u001b[0;36mpsd_array_welch\u001b[1;34m(x, sfreq, fmin, fmax, n_fft, n_overlap, n_per_seg, n_jobs, average, window, remove_dc, output, verbose)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;66;03m# Parallelize across first N-1 dimensions\u001b[39;00m\n\u001b[0;32m    204\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpectogram using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_fft\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-point FFT on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_per_seg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_overlap\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m overlap and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwindow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m window\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m )\n\u001b[1;32m--> 209\u001b[0m parallel, my_spect_func, n_jobs \u001b[38;5;241m=\u001b[39m \u001b[43mparallel_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_spect_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m _func \u001b[38;5;241m=\u001b[39m partial(\n\u001b[0;32m    211\u001b[0m     spectrogram,\n\u001b[0;32m    212\u001b[0m     detrend\u001b[38;5;241m=\u001b[39mdetrend,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    218\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    219\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(np\u001b[38;5;241m.\u001b[39misnan(x)):\n",
      "File \u001b[1;32m<decorator-gen-22>:12\u001b[0m, in \u001b[0;36mparallel_func\u001b[1;34m(func, n_jobs, max_nbytes, pre_dispatch, total, prefer, max_jobs, verbose)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mne\\parallel.py:95\u001b[0m, in \u001b[0;36mparallel_func\u001b[1;34m(func, n_jobs, max_nbytes, pre_dispatch, total, prefer, max_jobs, verbose)\u001b[0m\n\u001b[0;32m     92\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;66;03m# check if joblib is recent enough to support memmaping\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m     cache_dir \u001b[38;5;241m=\u001b[39m \u001b[43mget_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMNE_CACHE_DIR\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(max_nbytes, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m max_nbytes \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     97\u001b[0m         max_nbytes \u001b[38;5;241m=\u001b[39m get_config(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNE_MEMMAP_MIN_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mne\\utils\\config.py:302\u001b[0m, in \u001b[0;36mget_config\u001b[1;34m(key, default, raise_error, home_dir, use_env)\u001b[0m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron[key]\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# second, look for it in mne-python config file\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_config_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhome_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhome_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m op\u001b[38;5;241m.\u001b[39misfile(config_path):\n\u001b[0;32m    304\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mne\\utils\\config.py:252\u001b[0m, in \u001b[0;36mget_config_path\u001b[1;34m(home_dir)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_config_path\u001b[39m(home_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get path to standard mne-python config file.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m        system, this will be ~/.mne/mne-python.json.\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     val \u001b[38;5;241m=\u001b[39m op\u001b[38;5;241m.\u001b[39mjoin(\u001b[43m_get_extra_data_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhome_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhome_dir\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmne-python.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mne\\utils\\config.py:412\u001b[0m, in \u001b[0;36m_get_extra_data_path\u001b[1;34m(home_dir)\u001b[0m\n\u001b[0;32m    410\u001b[0m APPDATA_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAPPDATA\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    411\u001b[0m USERPROFILE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSERPROFILE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m APPDATA_DIR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAPPDATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.mne\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# backward-compat\u001b[39;00m\n\u001b[0;32m    415\u001b[0m     home_dir \u001b[38;5;241m=\u001b[39m APPDATA_DIR\n\u001b[0;32m    416\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m USERPROFILE_DIR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\29213\\AppData\\Local\\Programs\\Python\\Python39\\lib\\genericpath.py:42\u001b[0m, in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return true if the pathname refers to an existing directory.\"\"\"\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for subjects in subjectList:\n",
    "    FFT_Processing (subjects, channel, band, window_size, step_size, sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Segment of preprocessed data\n",
    "* training dataset: 75 %\n",
    "* validation dataset: 12.5%\n",
    "* testing dataset: 12.5%\n",
    "\n",
    "Agrithom pool:\n",
    "* Support Vector Machine (which kernal?)\n",
    "* Ada-Boost\n",
    "\n",
    "Best practice could be refered to this paper: \n",
    "\n",
    "@article{alarcao2017emotions,\n",
    "  title={Emotions recognition using EEG signals: A survey},\n",
    "  author={Alarcao, Soraia M and Fonseca, Manuel J},\n",
    "  journal={IEEE Transactions on Affective Computing},\n",
    "  year={2017},\n",
    "  publisher={IEEE}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for subjects in subjectList:\n",
    "data_training = []\n",
    "label_training = []\n",
    "data_testing = []\n",
    "label_testing = []\n",
    "data_validation = []\n",
    "label_validation = []\n",
    "\n",
    "for subjects in subjectList:\n",
    "\n",
    "    with open('out\\s' + subjects + '.npy', 'rb') as file:\n",
    "        sub = np.load(file)\n",
    "        for i in range (0,sub.shape[0]):\n",
    "            if i % 8 == 0:\n",
    "                data_testing.append(sub[i][0])\n",
    "                label_testing.append(sub[i][1])\n",
    "            elif i % 8 == 1:\n",
    "                data_validation.append(sub[i][0])\n",
    "                label_validation.append(sub[i][1])\n",
    "            else:\n",
    "                data_training.append(sub[i][0])\n",
    "                label_training.append(sub[i][1])\n",
    "\n",
    "np.save('out\\data_training', np.array(data_training), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_training', np.array(label_training), allow_pickle=True, fix_imports=True)\n",
    "print(\"training dataset:\", np.array(data_training).shape, np.array(label_training).shape)\n",
    "\n",
    "np.save('out\\data_testing', np.array(data_testing), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_testing', np.array(label_testing), allow_pickle=True, fix_imports=True)\n",
    "print(\"testing dataset:\", np.array(data_testing).shape, np.array(label_testing).shape)\n",
    "\n",
    "np.save('out\\data_validation', np.array(data_validation), allow_pickle=True, fix_imports=True)\n",
    "np.save('out\\label_validation', np.array(label_validation), allow_pickle=True, fix_imports=True)\n",
    "print(\"validation dataset:\", np.array(data_validation).shape, np.array(label_validation).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Regression\n",
    "### 0. Loading Training and Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out\\data_training.npy', 'rb') as fileTrain:\n",
    "    X  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_training.npy', 'rb') as fileTrainL:\n",
    "    Y  = np.load(fileTrainL)\n",
    "    \n",
    "X = normalize(X)\n",
    "Z = np.ravel(Y[:, [1]])\n",
    "\n",
    "Arousal_Train = np.ravel(Y[:, [0]])\n",
    "Valence_Train = np.ravel(Y[:, [1]])\n",
    "Domain_Train = np.ravel(Y[:, [2]])\n",
    "Like_Train = np.ravel(Y[:, [3]])\n",
    "\n",
    "\n",
    "\n",
    "with open('out\\data_validation.npy', 'rb') as fileTrain:\n",
    "    M  = np.load(fileTrain)\n",
    "    \n",
    "with open('out\\label_validation.npy', 'rb') as fileTrainL:\n",
    "    N  = np.load(fileTrainL)\n",
    "\n",
    "M = normalize(M)\n",
    "L = np.ravel(N[:, [1]])\n",
    "\n",
    "Arousal_Test = np.ravel(N[:, [0]])\n",
    "Valence_Test = np.ravel(N[:, [1]])\n",
    "Domain_Test = np.ravel(N[:, [2]])\n",
    "Like_Test = np.ravel(N[:, [3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Support Vector Regression\n",
    "* default setting, l1 error: 1.621761042477756 classification error: 0.6057377049180328 1478 2440"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVR()\n",
    "clf.fit(X[0:468480:32], Z[0:468480:32])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest Regression\n",
    "* n_estimators = 10, sample rate = 1/32, l1 error: 1.137919672131145 classification accuracy: 0.7774590163934426 1897 2440\n",
    "* n_estimators = 100, sample rate = 1/32, l1 error: 1.1029040163934432 classification accuracy: 0.8147540983606557 1988 2440\n",
    "* n_estimators = 100, min_samples_leaf=10, sample rate = 1/32, l1 error: 1.274458098574928 classification accuracy: 0.7622950819672131 1860 2440\n",
    "* n_estimators = 100, min_samples_leaf=50, sample rate = 1/32, l1 error: 1.4575897309409926 classification accuracy: 0.6823770491803278 1665 2440\n",
    "\n",
    "* n_estimators = 250, sample rate = 1/32, l1 error: 1.0905590819672137 classification accuracy: 0.830327868852459 2026 2440\n",
    "* n_estimators = 750, sample rate = 1/32, l1 error: 1.0953162021857932 classification accuracy: 0.8340163934426229 2035 2440\n",
    "* n_estimators = 750, sample rate = 1/8, l1 error: l1 error: 1.066982950819674 classification accuracy: 0.8217213114754098 2005 2440\n",
    "* __n_estimators = 512, sample rate = 1/32, l1 error: 1.092375304175206 classification accuracy: 0.8364754098360656 2041 2440\n",
    "__\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Val_R = RandomForestRegressor(n_estimators=512, n_jobs=6)\n",
    "Val_R.fit(X[0:468480:32], Valence_Train[0:468480:32])\n",
    "testing (M, Valence_Test, Val_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Aro_R = RandomForestRegressor(n_estimators=512, n_jobs=6)\n",
    "Aro_R.fit(X[0:468480:32], Arousal_Train[0:468480:32])\n",
    "testing (M, Arousal_Test, Aro_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dom_R = RandomForestRegressor(n_estimators=512, n_jobs=6)\n",
    "Dom_R.fit(X[0:468480:32], Domain_Train[0:468480:32])\n",
    "testing (M, Domain_Test, Dom_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Lik_R = RandomForestRegressor(n_estimators=512, n_jobs=6)\n",
    "Lik_R.fit(X[0:468480:32], Like_Train[0:468480:32])\n",
    "testing (M, Like_Test, Lik_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. AdaBoost Regression\n",
    "* n = 50, lr = 1.0: l2 error: 3.8454054839726695 classification accuracy: 0.6147540983606558 1500 2440\n",
    "* n = 50, lr = 1.0, square: l2 error: 4.015289218608164 classification accuracy: 0.5913934426229508 1443 2440\n",
    "* n = 500, lr = 1.0: l2 error: 3.8861651269012594 classification accuracy: 0.6155737704918033 1502 2440\n",
    "*\n",
    "*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = AdaBoostRegressor(n_estimators=5000, learning_rate=0.01)\n",
    "clf.fit(X[0:468480:32], Z[0:468480:32])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output = Val_R.predict(M[0:78080:32])\n",
    "label = L[0:78080:32]\n",
    "\n",
    "k = 0\n",
    "l = 0\n",
    "\n",
    "for i in range(len(label)):\n",
    "    k = k + (output[i] - label[i])*(output[i] - label[i]) #square difference \n",
    "    \n",
    "    #a good guess\n",
    "    if (output[i] > 5 and label[i] > 5):\n",
    "        l = l + 1\n",
    "    elif (output[i] < 5 and label[i] <5):\n",
    "        l = l + 1\n",
    "\n",
    "print (\"l2 error:\", k/len(label), \"classification accuracy:\", l / len(label),l, len(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ANN\n",
    "* 500 epoch 0.005 128 - 256 - 256 - 128 loss = 3.1\n",
    "* 3000 epoch 0.0001 256-512-512-256 Epoch: 3196 - Training Cost: 1.8372873067855835  Testing Cost: 2.231332540512085\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = X[0:468480:32]\n",
    "Y_training = Z[0:468480:32]\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = M[0:78080:32]\n",
    "Y_testing = L[0:78080:32]\n",
    "\n",
    "# DO Scale both the training inputs and outputs\n",
    "X_scaled_training = pd.DataFrame (data = X_training).values\n",
    "Y_scaled_training = pd.DataFrame (data = Y_training).values\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = pd.DataFrame (data = X_testing).values\n",
    "Y_scaled_testing = pd.DataFrame (data = Y_testing).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn off TensorFlow warning messages in program output\n",
    "from mne.utils.docs import tf\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Define model parameters\n",
    "t = time.time()\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 5000\n",
    "display_step = 1\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 70\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 512\n",
    "layer_2_nodes = 1024\n",
    "layer_3_nodes = 1024\n",
    "layer_4_nodes = 512\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "RUN_NAME = str(int(round(t * 1000))) + '_' + str(layer_1_nodes) + '_' + str(layer_2_nodes) + '_' + str(layer_3_nodes) + '_' + str(layer_4_nodes) + '_' + str(learning_rate) + '_' + str(training_epochs) + '_' + 'Val'\n",
    "\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Layer 4\n",
    "with tf.variable_scope('layer_4'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, layer_4_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[layer_4_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_4_output = tf.nn.relu(tf.matmul(layer_3_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights5\", shape=[layer_4_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases5\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_4_output, weights) + biases\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will be optimized during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./{}/logs/training\".format(RUN_NAME), session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./{}/logs/testing\".format(RUN_NAME), session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Every few training steps, log our progress\n",
    "        if epoch % display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n",
    "\n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "\n",
    "    save_path = saver.save(session, \"./{}/logs/trained_model.ckpt\".format(RUN_NAME))\n",
    "    print(\"Model saved: {}\".format(save_path))\n",
    "\n",
    "    '''\n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    \n",
    "'''\n",
    "    model_builder = tf.saved_model.builder.SavedModelBuilder(\"./{}/exported_model\".format(RUN_NAME))\n",
    "\n",
    "    inputs = {\n",
    "        'input': tf.saved_model.utils.build_tensor_info(X)\n",
    "        }\n",
    "    outputs = {\n",
    "        'earnings': tf.saved_model.utils.build_tensor_info(prediction)\n",
    "        }\n",
    "\n",
    "    signature_def = tf.saved_model.signature_def_utils.build_signature_def(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,\n",
    "        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME\n",
    "    )\n",
    "\n",
    "    model_builder.add_meta_graph_and_variables(\n",
    "        session,\n",
    "        tags=[tf.saved_model.tag_constants.SERVING],\n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY: signature_def\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model_builder.save()\n",
    "    print('model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
